import logging
from aiogram import Bot, Dispatcher, types
from aiogram.utils import executor
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

# ØªÙˆÙƒÙ† Ø§Ù„Ø¨ÙˆØª Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
TOKEN = os.getenv("7219331966:AAEHT6-IJO7DiAezb1J3sgtbKDrFhJ-kRoE")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Llama 2 Ø£Ùˆ Mistral)
model_name = "mistralai/Mistral-7B-Instruct"  # ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù€ Llama2
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙˆØª
bot = Bot(token=TOKEN)
dp = Dispatcher(bot)

logging.basicConfig(level=logging.INFO)

# Ø£Ù…Ø± /start
@dp.message_handler(commands=['start'])
async def send_welcome(message: types.Message):
    await message.reply("Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ø¨ÙˆØª Ø§Ù„Ø¥Ø®Ù„Ø§Ø¡! ğŸ¤\nØ£Ø±Ø³Ù„ Ø³Ø¤Ø§Ù„Ùƒ ÙˆØ³Ù†Ø±Ø¯ Ø¹Ù„ÙŠÙƒ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.")

# ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
async def get_ai_response(user_input):
    inputs = tokenizer(user_input, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ÙˆØ§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙ‡Ø§
@dp.message_handler()
async def handle_message(message: types.Message):
    user_text = message.text
    response = await get_ai_response(user_text)
    await message.reply(response)

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª
if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)
